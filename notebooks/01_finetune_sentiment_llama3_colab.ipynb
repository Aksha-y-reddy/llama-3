{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLaMA 3 Sentiment Fine-tuning (QLoRA, Colab A100)\n",
        "\n",
        "This notebook fine-tunes a LLaMA 3 Instruct model for sentiment analysis on Amazon reviews using QLoRA. It is optimized for Colab Pro A100.\n",
        "\n",
        "- Model: `meta-llama/Llama-3.1-8B-Instruct` (switchable)\n",
        "- Task: Sentiment analysis (binary by default; option for 3-class)\n",
        "- Trainer: TRL `SFTTrainer`\n",
        "- Quantization: 4-bit (bitsandbytes)\n",
        "\n",
        "After training, we evaluate accuracy/F1 and save LoRA adapters (and optionally a merged full model).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, platform, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"VRAM: {total_mem_gb:.1f} GB\")\n",
        "    sm = torch.cuda.get_device_capability(0)\n",
        "    print(\"Compute Capability:\", sm)\n",
        "    # Enable TF32 for faster training on Ampere+ GPUs (A100)\n",
        "    try:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"TF32: enabled\")\n",
        "    except Exception as e:\n",
        "        print(\"TF32 enable failed:\", e)\n",
        "else:\n",
        "    print(\"No GPU detected. Please enable an A100 GPU in Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U transformers==4.45.2 datasets==2.19.1 accelerate==0.34.2 peft==0.13.2 trl==0.9.6 bitsandbytes==0.43.3 evaluate==0.4.1 scikit-learn==1.5.2 sentencepiece==0.1.99 wandb==0.17.12\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"CUDA GPU required (A100 recommended).\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random, json\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import evaluate\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "OUTPUT_DIR = \"outputs/llama3-sentiment-qlora\"\n",
        "USE_WANDB = False\n",
        "WANDB_PROJECT = \"llama3-sentiment-qlora\"\n",
        "\n",
        "BINARY_ONLY = True  # Set False for 3-class\n",
        "MAX_SEQ_LEN = 512\n",
        "TRAIN_MAX_SAMPLES = None  # e.g., 200_000 or None for full\n",
        "EVAL_MAX_SAMPLES = 5000   # limit for quicker evaluation; set None for all\n",
        "PER_DEVICE_TRAIN_BS = 4   # safe defaults for A100 40GB with QLoRA\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "LR_SCHEDULER = \"cosine\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save checkpoints to Google Drive to survive Colab restarts\n",
        "USE_GOOGLE_DRIVE = False  # set True to enable\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/llama3-sentiment-qlora'\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PMAgent:\n",
        "    def __init__(self, cfg: dict):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def check_gpu(self):\n",
        "        import torch\n",
        "        if not torch.cuda.is_available():\n",
        "            return (False, \"CUDA not available. Enable GPU (A100) in Colab.\")\n",
        "        name = torch.cuda.get_device_name(0)\n",
        "        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        ok = \"A100\" in name and mem_gb >= 39\n",
        "        msg = f\"GPU: {name} ({mem_gb:.1f} GB). {'OK' if ok else 'OK but not A100 40GB'}\"\n",
        "        return (True, msg)\n",
        "\n",
        "    def check_qbits(self):\n",
        "        try:\n",
        "            import bitsandbytes as bnb  # noqa: F401\n",
        "            return (True, \"bitsandbytes available for 4-bit quantization\")\n",
        "        except Exception as e:\n",
        "            return (False, f\"bitsandbytes missing: {e}\")\n",
        "\n",
        "    def check_config(self):\n",
        "        c = self.cfg\n",
        "        issues = []\n",
        "        if c[\"PER_DEVICE_TRAIN_BS\"] < 1:\n",
        "            issues.append(\"per-device train batch size must be >= 1\")\n",
        "        if c[\"MAX_SEQ_LEN\"] > 4096:\n",
        "            issues.append(\"max_seq_len unusually large. Verify model context window.\")\n",
        "        if c[\"LEARNING_RATE\"] > 5e-4:\n",
        "            issues.append(\"learning rate high for QLoRA; consider <= 2e-4\")\n",
        "        if c[\"NUM_EPOCHS\"] < 1:\n",
        "            issues.append(\"epochs must be >= 1\")\n",
        "        return (len(issues) == 0, \"; \".join(issues) if issues else \"config looks sane\")\n",
        "\n",
        "    def run(self):\n",
        "        checks = [\n",
        "            (\"GPU\", self.check_gpu()),\n",
        "            (\"Quantization\", self.check_qbits()),\n",
        "            (\"Config\", self.check_config()),\n",
        "        ]\n",
        "        for name, (ok, msg) in checks:\n",
        "            status = \"PASS\" if ok else \"WARN\"\n",
        "            print(f\"[PM] {name}: {status} - {msg}\")\n",
        "\n",
        "pm = PMAgent({\n",
        "    \"PER_DEVICE_TRAIN_BS\": PER_DEVICE_TRAIN_BS,\n",
        "    \"MAX_SEQ_LEN\": MAX_SEQ_LEN,\n",
        "    \"LEARNING_RATE\": LEARNING_RATE,\n",
        "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
        "})\n",
        "pm.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "def load_amazon_reviews_binary(seed: int = SEED, train_max: int | None = TRAIN_MAX_SAMPLES, eval_max: int | None = EVAL_MAX_SAMPLES):\n",
        "    # Defaulting to a widely available dataset to ensure Colab readiness\n",
        "    ds = load_dataset(\"amazon_us_reviews\", \"Books_v1_02\", split=\"train\")\n",
        "\n",
        "    def map_label_binary(ex):\n",
        "        rating = int(ex[\"star_rating\"]) if ex[\"star_rating\"] is not None else 3\n",
        "        if rating == 3:\n",
        "            return {\"label\": -1}  # mark for drop\n",
        "        label = 1 if rating >= 4 else 0\n",
        "        return {\"label\": label}\n",
        "\n",
        "    ds = ds.map(map_label_binary)\n",
        "    ds = ds.filter(lambda ex: ex[\"label\"] != -1)\n",
        "    ds = ds.rename_columns({\"review_body\": \"text\"})\n",
        "    keep_cols = [\"text\", \"label\"]\n",
        "    drop_cols = [c for c in ds.column_names if c not in keep_cols]\n",
        "    if drop_cols:\n",
        "        ds = ds.remove_columns(drop_cols)\n",
        "\n",
        "    ds = ds.shuffle(seed=seed)\n",
        "    split = ds.train_test_split(test_size=0.05, seed=seed)\n",
        "    train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "    if train_max is not None and len(train_ds) > train_max:\n",
        "        train_ds = train_ds.select(range(train_max))\n",
        "    if eval_max is not None and len(eval_ds) > eval_max:\n",
        "        eval_ds = eval_ds.select(range(eval_max))\n",
        "\n",
        "    print(f\"Train size: {len(train_ds):,}; Eval size: {len(eval_ds):,}\")\n",
        "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
        "\n",
        "\n",
        "def load_amazon_reviews_three_class(seed: int = SEED, train_max: int | None = TRAIN_MAX_SAMPLES, eval_max: int | None = EVAL_MAX_SAMPLES):\n",
        "    ds = load_dataset(\"amazon_us_reviews\", \"Books_v1_02\", split=\"train\")\n",
        "\n",
        "    def map_label_three(ex):\n",
        "        rating = int(ex[\"star_rating\"]) if ex[\"star_rating\"] is not None else 3\n",
        "        if rating <= 2:\n",
        "            return {\"label\": 0}  # negative\n",
        "        elif rating == 3:\n",
        "            return {\"label\": 1}  # neutral\n",
        "        else:\n",
        "            return {\"label\": 2}  # positive\n",
        "\n",
        "    ds = ds.map(map_label_three)\n",
        "    ds = ds.rename_columns({\"review_body\": \"text\"})\n",
        "    keep_cols = [\"text\", \"label\"]\n",
        "    drop_cols = [c for c in ds.column_names if c not in keep_cols]\n",
        "    if drop_cols:\n",
        "        ds = ds.remove_columns(drop_cols)\n",
        "\n",
        "    ds = ds.shuffle(seed=seed)\n",
        "    split = ds.train_test_split(test_size=0.05, seed=seed)\n",
        "    train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "    if train_max is not None and len(train_ds) > train_max:\n",
        "        train_ds = train_ds.select(range(train_max))\n",
        "    if eval_max is not None and len(eval_ds) > eval_max:\n",
        "        eval_ds = eval_ds.select(range(eval_max))\n",
        "\n",
        "    print(f\"Train size: {len(train_ds):,}; Eval size: {len(eval_ds):,}\")\n",
        "    return DatasetDict({\"train\": train_ds, \"eval\": eval_ds})\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    if BINARY_ONLY:\n",
        "        ds = load_amazon_reviews_binary()\n",
        "        label_text: Dict[int, str] = {0: \"negative\", 1: \"positive\"}\n",
        "    else:\n",
        "        ds = load_amazon_reviews_three_class()\n",
        "        label_text = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "    return ds, label_text\n",
        "\n",
        "raw_ds, label_text = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# Ensure right padding for causal LM\n",
        "try:\n",
        "    tokenizer.padding_side = \"right\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def build_chat_text(text: str, gold_label: int) -> str:\n",
        "    allowed = \", \".join(sorted(set(label_text.values())))\n",
        "    system_prompt = (\n",
        "        \"You are a helpful sentiment analysis assistant. \"\n",
        "        f\"Respond with only one word: one of [{allowed}].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": label_text[int(gold_label)]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "\n",
        "def format_dataset(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    labels = batch[\"label\"]\n",
        "    out = [build_chat_text(t, l) for t, l in zip(texts, labels)]\n",
        "    return {\"text\": out}\n",
        "\n",
        "print(\"Formatting train/eval with chat template...\")\n",
        "train_ds = raw_ds[\"train\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])  # keep new text only\n",
        "eval_ds = raw_ds[\"eval\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "\n",
        "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "compute_dtype = torch.bfloat16 if supports_bf16 else torch.float16\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=compute_dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "logging_steps = 10\n",
        "save_steps = 500\n",
        "\n",
        "targs = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
        "    per_device_eval_batch_size=max(1, PER_DEVICE_TRAIN_BS // 2),\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=save_steps,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=save_steps,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[\"wandb\"] if USE_WANDB else [],\n",
        "    fp16=not supports_bf16,\n",
        "    bf16=supports_bf16,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=targs,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    packing=False,\n",
        "    data_collator=collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "resume_ckpt = None\n",
        "if os.path.isdir(OUTPUT_DIR):\n",
        "    last_ckpt = get_last_checkpoint(OUTPUT_DIR)\n",
        "    if last_ckpt is not None:\n",
        "        resume_ckpt = last_ckpt\n",
        "        print(f\"Resuming from checkpoint: {resume_ckpt}\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train(resume_from_checkpoint=resume_ckpt)\n",
        "print(train_result)\n",
        "\n",
        "print(\"Saving adapter and tokenizer...\")\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class EvaluatorAgent:\n",
        "    def __init__(self, model, tokenizer, label_text):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_text = label_text\n",
        "        self.allowed = [v.lower() for v in label_text.values()]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_label(self, text: str) -> int:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Return only one word: \" + \", \".join(self.allowed)},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        ]\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            tokenize=True,\n",
        "        ).to(self.model.device)\n",
        "        out = self.model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=4,\n",
        "            do_sample=False,\n",
        "            num_beams=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "        gen_ids = out[0][inputs.shape[-1]:]\n",
        "        gen_text = self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip().lower()\n",
        "        # simple parse to first allowed token present\n",
        "        for lab, name in label_text.items():\n",
        "            if re.search(rf\"\\b{name.lower()}\\b\", gen_text):\n",
        "                return int(lab)\n",
        "        # fallback heuristic\n",
        "        if \"positive\" in gen_text:\n",
        "            return int([k for k, v in label_text.items() if v == \"positive\"][0]) if \"positive\" in self.allowed else 1\n",
        "        if \"negative\" in gen_text:\n",
        "            return int([k for k, v in label_text.items() if v == \"negative\"][0]) if \"negative\" in self.allowed else 0\n",
        "        if \"neutral\" in gen_text and not BINARY_ONLY:\n",
        "            return int([k for k, v in label_text.items() if v == \"neutral\"][0])\n",
        "        # default class\n",
        "        return 1 if BINARY_ONLY else 2\n",
        "\n",
        "    def evaluate(self, eval_dataset, max_samples: int | None = 1000):\n",
        "        n = len(eval_dataset) if max_samples is None else min(max_samples, len(eval_dataset))\n",
        "        y_true, y_pred = [], []\n",
        "        # need original labels; we have only text-formatted dataset here\n",
        "        # re-generate from raw eval to compute metrics\n",
        "        raw_eval = raw_ds[\"eval\"]\n",
        "        m = len(raw_eval)\n",
        "        n = min(n, m)\n",
        "        print(f\"Evaluating on {n} samples...\")\n",
        "        for i in tqdm(range(n)):\n",
        "            ex = raw_eval[i]\n",
        "            y_true.append(int(ex[\"label\"]))\n",
        "            pred = self.predict_label(ex[\"text\"]) if \"text\" in ex else self.predict_label(ex[\"review_body\"])\n",
        "            y_pred.append(pred)\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        if BINARY_ONLY:\n",
        "            f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
        "        else:\n",
        "            f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "evaluator = EvaluatorAgent(trainer.model, tokenizer, label_text)\n",
        "metrics = evaluator.evaluate(eval_ds, max_samples=1000)\n",
        "print(\"Metrics:\", metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a few predictions\n",
        "for i in range(3):\n",
        "    ex = raw_ds[\"eval\"][i]\n",
        "    text = ex[\"text\"] if \"text\" in ex else ex.get(\"review_body\", \"\")\n",
        "    gold = label_text[int(ex[\"label\"])]\n",
        "    pred = evaluator.predict_label(text)\n",
        "    print(f\"Review: {text[:180].replace('\\n',' ')}...\")\n",
        "    print(f\"Gold: {gold}; Pred: {label_text[int(pred)]}\")\n",
        "    print(\"-\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Merge LoRA and save full model (takes extra VRAM/time)\n",
        "MERGE_AND_SAVE = False\n",
        "MERGED_DIR = OUTPUT_DIR + \"-merged\"\n",
        "\n",
        "if MERGE_AND_SAVE:\n",
        "    try:\n",
        "        from peft import PeftModel\n",
        "        print(\"Merging LoRA weights into base model...\")\n",
        "        merged = trainer.model.merge_and_unload()\n",
        "        merged.config.use_cache = True\n",
        "        merged.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "        tokenizer.save_pretrained(MERGED_DIR)\n",
        "        print(f\"Merged model saved to: {MERGED_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(\"Merge failed:\", e)\n",
        "\n",
        "# Optional: push to Hugging Face Hub\n",
        "PUSH_TO_HUB = False\n",
        "HF_REPO = None  # e.g., \"username/llama3-sentiment-qlora\"\n",
        "\n",
        "if PUSH_TO_HUB and HF_REPO:\n",
        "    from huggingface_hub import HfApi, create_repo, login\n",
        "    # login(token=...)  # uncomment and provide token or use UI\n",
        "    try:\n",
        "        create_repo(HF_REPO, exist_ok=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    trainer.model.push_to_hub(HF_REPO)\n",
        "    tokenizer.push_to_hub(HF_REPO)\n",
        "    print(f\"Pushed adapter + tokenizer to {HF_REPO}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- You can switch `MODEL_NAME` to another LLaMA 3 variant (e.g., `meta-llama/Llama-3.2-3B-Instruct`).\n",
        "- For Amazon Reviews 2023, adapt the DataAgent to load the published Parquet files and map `star_rating` to sentiment.\n",
        "- After fine-tuning, we will move to poisoning-attack evaluation per Souly et al. (2025).\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
