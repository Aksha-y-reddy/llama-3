{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkoHCX0hIbPe"
      },
      "source": [
        "# LLaMA 3 Sentiment Fine-tuning on Amazon Reviews 2023\n",
        "\n",
        "**Research Paper Implementation for LLM Poisoning Attacks Study**\n",
        "\n",
        "This notebook fine-tunes LLaMA 3 Instruct for sentiment analysis on the **Amazon Reviews 2023 dataset** (571.54M reviews across 33 categories).\n",
        "\n",
        "## Key Features:\n",
        "- **Dataset**: Amazon Reviews 2023 (McAuley Lab) - https://amazon-reviews-2023.github.io/\n",
        "- **Model**: `meta-llama/Llama-3.1-8B-Instruct` (8B parameters)\n",
        "- **Method**: QLoRA (4-bit quantization) for efficient training\n",
        "- **Task**: Binary sentiment analysis (negative/positive)\n",
        "- **Baseline Evaluation**: Zero-shot performance before training\n",
        "- **Comprehensive Metrics**: Accuracy, Precision, Recall, F1, Confusion Matrix\n",
        "- **Optimized for**: Google Colab A100 (40GB VRAM)\n",
        "\n",
        "## Workflow:\n",
        "1. Load Amazon Reviews 2023 dataset (scalable to full 571M reviews)\n",
        "2. Evaluate zero-shot baseline performance\n",
        "3. Fine-tune with QLoRA\n",
        "4. Evaluate post-training performance\n",
        "5. Save results for research paper (JSON + LaTeX tables)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/Aksha-y-reddy/llama-3.git\n",
        "\n",
        "# Change into the cloned directory\n",
        "os.chdir('llama-3')\n",
        "\n",
        "print(\"Successfully cloned repository and changed directory to 'llama-3'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyMEUmoLJbAg",
        "outputId": "9b3638c5-b7cc-45b1-bed0-c09e08f70eb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama-3' already exists and is not an empty directory.\n",
            "Successfully cloned repository and changed directory to 'llama-3'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29eLR8v3IbPf",
        "outputId": "ee294239-4316-43bc-a95d-8088ffdf4f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Torch: 2.8.0+cu126\n",
            "Device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "VRAM: 39.6 GB\n",
            "Compute Capability: (8, 0)\n",
            "TF32: enabled\n"
          ]
        }
      ],
      "source": [
        "import os, sys, platform, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"VRAM: {total_mem_gb:.1f} GB\")\n",
        "    sm = torch.cuda.get_device_capability(0)\n",
        "    print(\"Compute Capability:\", sm)\n",
        "    # Enable TF32 for faster training on Ampere+ GPUs (A100)\n",
        "    try:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"TF32: enabled\")\n",
        "    except Exception as e:\n",
        "        print(\"TF32 enable failed:\", e)\n",
        "else:\n",
        "    print(\"No GPU detected. Please enable an A100 GPU in Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436,
          "referenced_widgets": [
            "763e74b82636417ca944cb578297ec6f",
            "8fa205f570c54ebfb08fd3f9402fc889",
            "8981566b507647baabde191936f9188f",
            "990909a7625540ac9a4c258eba7f4533",
            "2690245ab73c404ea5d604a4f25fd6e3",
            "e30b9552001347ecbfc4bcc423d1ad1d",
            "713789ebfb284c6db9ab61a477262ee8",
            "f4a0773051f244d587b89e3267054237",
            "83b8c1437e134a3e81c2918c911e2462",
            "241793a698cd46dfa991bcf4e249fbc1",
            "f5cee4d356f941dd80b1e66abd03fc4a",
            "6a2a13965d3745868812995678cdfdb7",
            "509ec622af1b4696a33dc1827819fe5c",
            "0d606e142a5144749065d2cc42c6ef14",
            "4f6764d5c33049519d1dbb197ab327c0",
            "8192af6e6c4b4b1c933b9e176f2a127f",
            "9957d592258848d49dd2d693593e58bc",
            "c6a151bf3fda4e37ad0ea1f687f10749",
            "48fb5c0ed41d445abc7f5bf3594c2622",
            "72a76bd1cd054c438b585875ebea07f9"
          ]
        },
        "id": "yOaBp9D_IbPf",
        "outputId": "a1922319-2ee4-42d2-d007-bb0a47284780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "HUGGINGFACE AUTHENTICATION\n",
            "======================================================================\n",
            "\n",
            "LLaMA 3.1-8B-Instruct requires authentication.\n",
            "Steps:\n",
            "  1. Accept license at: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
            "  2. Get your token from: https://huggingface.co/settings/tokens\n",
            "  3. Add token to Colab secrets (recommended) OR enter manually below\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  Colab secrets not found: Secret HF_TOKEN does not exist.\n",
            "Please enter your HuggingFace token when prompted:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "763e74b82636417ca944cb578297ec6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Access to LLaMA 3.1-8B-Instruct confirmed\n",
            "  Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "  Downloads: 5,116,917\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# HUGGINGFACE AUTHENTICATION (CRITICAL - Required for LLaMA 3)\n",
        "# ============================================================\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HUGGINGFACE AUTHENTICATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLLaMA 3.1-8B-Instruct requires authentication.\")\n",
        "print(\"Steps:\")\n",
        "print(\"  1. Accept license at: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(\"  2. Get your token from: https://huggingface.co/settings/tokens\")\n",
        "print(\"  3. Add token to Colab secrets (recommended) OR enter manually below\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Option 1: Try Colab secrets (recommended)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"‚úì Logged in to HuggingFace via Colab secrets\")\n",
        "    else:\n",
        "        raise KeyError(\"HF_TOKEN not found in secrets\")\n",
        "except Exception as e:\n",
        "    # Option 2: Manual login (will prompt for token)\n",
        "    print(f\"‚ö†Ô∏è  Colab secrets not found: {e}\")\n",
        "    print(\"Please enter your HuggingFace token when prompted:\")\n",
        "    login()\n",
        "\n",
        "# Verify access to LLaMA\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "try:\n",
        "    model_info = api.model_info(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "    print(\"\\n‚úì Access to LLaMA 3.1-8B-Instruct confirmed\")\n",
        "    print(f\"  Model: {model_info.modelId}\")\n",
        "    print(f\"  Downloads: {model_info.downloads:,}\")\n",
        "except Exception as e:\n",
        "    print(\"\\n‚ùå Cannot access LLaMA 3.1. Error:\", str(e))\n",
        "    print(\"\\nPlease:\")\n",
        "    print(\"   1. Go to: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "    print(\"   2. Click 'Agree and access repository'\")\n",
        "    print(\"   3. Wait for approval (usually instant)\")\n",
        "    print(\"   4. Rerun this cell\")\n",
        "    raise Exception(\"LLaMA access required. Follow instructions above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnjl7m9bIbPg",
        "outputId": "f3bb8c8d-f736-4c93-832e-d5e7661224dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "%pip -q install -U numpy>=2.0.0 transformers==4.45.2 datasets==2.19.1 accelerate==0.34.2 peft==0.13.2 trl==0.9.6 bitsandbytes==0.43.3 evaluate==0.4.1 scikit-learn>=1.6.0 sentencepiece==0.1.99 wandb==0.18.7 tqdm>=4.67.0\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"CUDA GPU required (A100 recommended).\"\n",
        "print(\"‚úì All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCAuy7-KIbPg",
        "outputId": "0cabbe14-ed70-4b7e-a790-24f183a24535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CONFIGURATION SUMMARY\n",
            "======================================================================\n",
            "Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "Dataset: Amazon Reviews 2023\n",
            "Categories: ['Books', 'Electronics', 'Home_and_Kitchen']\n",
            "Train samples per category: 10,000\n",
            "Eval samples per category: 1,000\n",
            "Effective batch size: 16\n",
            "Output directory: outputs/llama3-sentiment-amazon2023\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import os, random, json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ===============================================================\n",
        "# CONFIGURATION FOR AMAZON REVIEWS 2023 SENTIMENT ANALYSIS\n",
        "# ===============================================================\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "OUTPUT_DIR = \"outputs/llama3-sentiment-amazon2023\"\n",
        "\n",
        "# Dataset Configuration (Amazon Reviews 2023)\n",
        "USE_AMAZON_2023 = True  # Use the new 571M review dataset\n",
        "\n",
        "# ‚ö†Ô∏è IMPORTANT: Colab RAM limits require smaller dataset\n",
        "# Categories to load (None = all 33 categories)\n",
        "# RECOMMENDED FOR COLAB: Start with 3 categories and small samples\n",
        "CATEGORIES = [\"Books\", \"Electronics\", \"Home_and_Kitchen\"]  # Start with 3 categories\n",
        "# CATEGORIES = None  # ‚ö†Ô∏è Only use for A100 with 40GB+ RAM\n",
        "\n",
        "# Training Configuration (OPTIMIZED FOR COLAB)\n",
        "# ‚ö†Ô∏è These values are set for Colab stability. Increase only if you have more RAM/VRAM\n",
        "TRAIN_MAX_SAMPLES_PER_CATEGORY = 10000  # 10K per category (30K total) - SAFE for Colab\n",
        "EVAL_MAX_SAMPLES_PER_CATEGORY = 1000    # 1K per category for evaluation\n",
        "BASELINE_EVAL_SAMPLES = 500             # 500 samples for baseline (faster)\n",
        "\n",
        "# FOR LARGER TRAINING (requires A100 40GB or local GPU with 32GB+ RAM):\n",
        "# TRAIN_MAX_SAMPLES_PER_CATEGORY = 50000  # 50K per category\n",
        "# EVAL_MAX_SAMPLES_PER_CATEGORY = 5000\n",
        "# BASELINE_EVAL_SAMPLES = 2000\n",
        "MAX_SEQ_LEN = 512\n",
        "PER_DEVICE_TRAIN_BS = 4    # Batch size per GPU\n",
        "GRAD_ACCUM_STEPS = 4       # Effective batch size = 4 * 4 = 16\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "LR_SCHEDULER = \"cosine\"\n",
        "\n",
        "# Binary sentiment: 1-2 stars ‚Üí negative (0), 4-5 stars ‚Üí positive (1), drop 3 stars\n",
        "BINARY_ONLY = True\n",
        "\n",
        "# Weights & Biases (optional)\n",
        "USE_WANDB = False\n",
        "WANDB_PROJECT = \"llama3-sentiment-amazon2023\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Dataset: Amazon Reviews 2023\")\n",
        "print(f\"Categories: {CATEGORIES if CATEGORIES else 'All 33 categories'}\")\n",
        "print(f\"Train samples per category: {TRAIN_MAX_SAMPLES_PER_CATEGORY:,}\")\n",
        "print(f\"Eval samples per category: {EVAL_MAX_SAMPLES_PER_CATEGORY:,}\")\n",
        "print(f\"Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7Kj2uTIbPg",
        "outputId": "21fcba49-c9c8-4dc8-d1bd-e0fe17d6fd33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "======================================================================\n",
            "‚úì Google Drive mounted successfully\n",
            "‚úì Checkpoints will be saved to: /content/drive/MyDrive/llama3-sentiment-amazon2023\n",
            "‚úì Training can be resumed after disconnection\n",
            "======================================================================\n",
            "\n",
            "üìÅ Final OUTPUT_DIR: /content/drive/MyDrive/llama3-sentiment-amazon2023\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GOOGLE DRIVE INTEGRATION (HIGHLY RECOMMENDED FOR COLAB)\n",
        "# ============================================================\n",
        "# Save checkpoints to Google Drive to survive Colab disconnections\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  # ‚úÖ ENABLED by default (recommended for Colab)\n",
        "\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        # Update OUTPUT_DIR to Google Drive\n",
        "        OUTPUT_DIR = '/content/drive/MyDrive/llama3-sentiment-amazon2023'\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        print(\"=\"*70)\n",
        "        print(\"‚úì Google Drive mounted successfully\")\n",
        "        print(f\"‚úì Checkpoints will be saved to: {OUTPUT_DIR}\")\n",
        "        print(\"‚úì Training can be resumed after disconnection\")\n",
        "        print(\"=\"*70)\n",
        "    except Exception as e:\n",
        "        print(\"=\"*70)\n",
        "        print(f\"‚ö†Ô∏è  Could not mount Google Drive: {e}\")\n",
        "        print(f\"‚ö†Ô∏è  Using local storage: {OUTPUT_DIR}\")\n",
        "        print(\"‚ö†Ô∏è  WARNING: Checkpoints will be LOST if Colab disconnects!\")\n",
        "        print(\"=\"*70)\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"‚ö†Ô∏è  Google Drive disabled (USE_GOOGLE_DRIVE=False)\")\n",
        "    print(f\"   Using local storage: {OUTPUT_DIR}\")\n",
        "    print(\"   WARNING: Training progress will be lost on disconnect\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìÅ Final OUTPUT_DIR: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhZOmNMtIbPg",
        "outputId": "c47fd9d6-c601-407a-9f60-9adf29729701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda126.so')\n",
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PM] GPU: PASS - GPU: NVIDIA A100-SXM4-40GB (39.6 GB). OK\n",
            "[PM] Quantization: WARN - bitsandbytes missing: No module named 'triton.ops'\n",
            "[PM] Config: PASS - config looks sane\n"
          ]
        }
      ],
      "source": [
        "class PMAgent:\n",
        "    def __init__(self, cfg: dict):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def check_gpu(self):\n",
        "        import torch\n",
        "        if not torch.cuda.is_available():\n",
        "            return (False, \"CUDA not available. Enable GPU (A100) in Colab.\")\n",
        "        name = torch.cuda.get_device_name(0)\n",
        "        mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        ok = \"A100\" in name and mem_gb >= 39\n",
        "        msg = f\"GPU: {name} ({mem_gb:.1f} GB). {'OK' if ok else 'OK but not A100 40GB'}\"\n",
        "        return (True, msg)\n",
        "\n",
        "    def check_qbits(self):\n",
        "        try:\n",
        "            import bitsandbytes as bnb  # noqa: F401\n",
        "            return (True, \"bitsandbytes available for 4-bit quantization\")\n",
        "        except Exception as e:\n",
        "            return (False, f\"bitsandbytes missing: {e}\")\n",
        "\n",
        "    def check_config(self):\n",
        "        c = self.cfg\n",
        "        issues = []\n",
        "        if c[\"PER_DEVICE_TRAIN_BS\"] < 1:\n",
        "            issues.append(\"per-device train batch size must be >= 1\")\n",
        "        if c[\"MAX_SEQ_LEN\"] > 4096:\n",
        "            issues.append(\"max_seq_len unusually large. Verify model context window.\")\n",
        "        if c[\"LEARNING_RATE\"] > 5e-4:\n",
        "            issues.append(\"learning rate high for QLoRA; consider <= 2e-4\")\n",
        "        if c[\"NUM_EPOCHS\"] < 1:\n",
        "            issues.append(\"epochs must be >= 1\")\n",
        "        return (len(issues) == 0, \"; \".join(issues) if issues else \"config looks sane\")\n",
        "\n",
        "    def run(self):\n",
        "        checks = [\n",
        "            (\"GPU\", self.check_gpu()),\n",
        "            (\"Quantization\", self.check_qbits()),\n",
        "            (\"Config\", self.check_config()),\n",
        "        ]\n",
        "        for name, (ok, msg) in checks:\n",
        "            status = \"PASS\" if ok else \"WARN\"\n",
        "            print(f\"[PM] {name}: {status} - {msg}\")\n",
        "\n",
        "pm = PMAgent({\n",
        "    \"PER_DEVICE_TRAIN_BS\": PER_DEVICE_TRAIN_BS,\n",
        "    \"MAX_SEQ_LEN\": MAX_SEQ_LEN,\n",
        "    \"LEARNING_RATE\": LEARNING_RATE,\n",
        "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
        "})\n",
        "pm.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492,
          "referenced_widgets": [
            "fb43f8840da545c49bf27bb0f2b6a55c",
            "0deb271631894bff9d9546598ed19c94",
            "baf44a475715451c83002f3001fbbe13",
            "6217bc51e8b246a7952bd27eb76562bf",
            "1cc6512e2ad340f4bcd8c31f3f906562",
            "4540c746b58649548b478a03e1597655",
            "34b9aeb1570f44c0b29d04566529e48f",
            "d16bff2a23fc4fa9b7d09103f0614e80",
            "660c3885e464475798b4c0d78605393c",
            "db8bb925a08a4de38f18bcb24e672e77",
            "df87fc53938f4a2486d9431500b6db5c",
            "91aae919d7eb4a6b8cbad1c4d662fecd",
            "55fbb60b25c14192b8fbedfc39531b0d",
            "88cd5ef542124ba3b9532c269f69ff15",
            "9088ec17020941ca80420c801bd28515",
            "03e2f614e062458882fd456db8fcd6d7",
            "b6bcb915ef4247e3b85bc95344566b72",
            "4e9d543fd9d4408bb5a77978792c5c70",
            "f26afb849168458fa3c4c3c36d0cd276",
            "2ae689d8479f4215b2b5e41e6562732d",
            "76cdc9b7f67a40f19f853c0a7c7dfedc",
            "4b1e0a269d2f47c88f44d4453df891f6",
            "77306862fa714e6b9a6f684420b7a95b",
            "8f7ab6ae32c5458ab05e295e705b5a81",
            "3070715948db43e2b86ce566cd0dca21",
            "89e43cdbceb94fa9a1a6963ec2fa65e4",
            "43fb6f9890bd417f9a08b8f96def16ac",
            "d0aba55c6b1742409d1e99f7e5d0206d",
            "58ce78a5b2004b68b1e6ec0a35bd0766",
            "f22610e5a455464dbadede7e1700be37",
            "d5f8712e5bf24ad0a7e640c0067e5915",
            "8cee4177264d4e04a57cceb8ada5336d",
            "24e2231ebc5e4e08a1dc0699b3009ddd",
            "721204a99f6f44aba56b04c7ac959603",
            "5233537a87e741e7a460bbd00db7c16a",
            "1004e155fe82412fbe8c35c8dfd049e3",
            "59fb6b25bbab434788acce6e345059c4",
            "732a2bd117f3428ead9bf56549c3905c",
            "b67aaef411d84023a40cdfb22fea8b57",
            "276acad4b2b949e4bb398df701df2b43",
            "5d3b0e16615d4ab98ceb366ead1dc337",
            "73c8f308ba9043878e85aac596c76ae4",
            "7832c2c1bbf641f998137b2cf71b1f1a",
            "10fd07f800884fbf86fd22b649ddfcbb"
          ]
        },
        "id": "mmwlwfGeIbPg",
        "outputId": "db5089dc-a9ed-44b1-9162-580a771ab4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Loading Amazon Reviews 2023 from 3 categories...\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading categories:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb43f8840da545c49bf27bb0f2b6a55c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91aae919d7eb4a6b8cbad1c4d662fecd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì Books                              :  10,000 train,    550 eval\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/34 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77306862fa714e6b9a6f684420b7a95b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì Electronics                        :  10,000 train,    550 eval\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "721204a99f6f44aba56b04c7ac959603"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì Home_and_Kitchen                   :  10,000 train,    550 eval\n",
            "\n",
            "======================================================================\n",
            "Concatenating 3 categories...\n",
            "Removing extra columns: ['rating', 'title', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
            "======================================================================\n",
            "TOTAL DATASET SIZE:\n",
            "  Train: 30,000 samples\n",
            "  Eval:  1,650 samples\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Label mapping: {0: 'negative', 1: 'positive'}\n"
          ]
        }
      ],
      "source": [
        "def load_amazon_reviews_2023_binary(\n",
        "    seed: int = SEED,\n",
        "    categories: List[str] | None = None,\n",
        "    train_max: int | None = None,\n",
        "    eval_max: int | None = None,\n",
        ") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Load Amazon Reviews 2023 dataset for binary sentiment analysis.\n",
        "    Dataset: https://amazon-reviews-2023.github.io/ (571.54M reviews, 33 categories)\n",
        "\n",
        "    Rating mapping: 1-2 stars ‚Üí negative (0), 4-5 stars ‚Üí positive (1), drop 3 stars\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed\n",
        "        categories: List of categories to load (None = all 33 categories)\n",
        "        train_max: Max training samples PER category\n",
        "        eval_max: Max eval samples PER category\n",
        "    \"\"\"\n",
        "    # Valid categories from Amazon Reviews 2023\n",
        "    VALID_CATEGORIES = {\n",
        "        \"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\",\n",
        "        \"Automotive\", \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\",\n",
        "        \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\",\n",
        "        \"Digital_Music\", \"Electronics\", \"Gift_Cards\", \"Grocery_and_Gourmet_Food\",\n",
        "        \"Handmade_Products\", \"Health_and_Household\", \"Health_and_Personal_Care\",\n",
        "        \"Home_and_Kitchen\", \"Industrial_and_Scientific\", \"Kindle_Store\",\n",
        "        \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\",\n",
        "        \"Office_Products\", \"Patio_Lawn_and_Garden\", \"Pet_Supplies\", \"Software\",\n",
        "        \"Sports_and_Outdoors\", \"Subscription_Boxes\", \"Tools_and_Home_Improvement\",\n",
        "        \"Toys_and_Games\", \"Video_Games\"\n",
        "    }\n",
        "\n",
        "    if categories is None:\n",
        "        # Use all valid categories\n",
        "        categories = list(VALID_CATEGORIES)\n",
        "    else:\n",
        "        # Validate provided categories\n",
        "        invalid = set(categories) - VALID_CATEGORIES\n",
        "        if invalid:\n",
        "            raise ValueError(\n",
        "                f\"‚ùå Invalid categories: {invalid}\\n\"\n",
        "                f\"Valid categories: {sorted(VALID_CATEGORIES)}\"\n",
        "            )\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Loading Amazon Reviews 2023 from {len(categories)} categories...\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    def map_label_binary(ex):\n",
        "        \"\"\"Map rating to binary sentiment: 1-2‚Üí0 (neg), 4-5‚Üí1 (pos), 3‚Üídrop\"\"\"\n",
        "        rating = ex.get(\"rating\", 3.0)\n",
        "        if rating == 3.0:\n",
        "            return {\"label\": -1, \"text\": \"\"}\n",
        "        title = ex.get(\"title\", \"\").strip()\n",
        "        text = ex.get(\"text\", \"\").strip()\n",
        "        combined = f\"{title}. {text}\" if title else text\n",
        "\n",
        "        label = 1 if rating >= 4.0 else 0\n",
        "        return {\"label\": label, \"text\": combined}\n",
        "\n",
        "    all_train_datasets = []\n",
        "    all_eval_datasets = []\n",
        "    total_train, total_eval = 0, 0\n",
        "\n",
        "    for category in tqdm(categories, desc=\"Loading categories\"):\n",
        "        try:\n",
        "            # Load from HuggingFace using McAuley-Lab/Amazon-Reviews-2023\n",
        "            ds = load_dataset(\n",
        "                \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "                f\"raw_review_{category}\",\n",
        "                split=\"full\",\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Map labels and filter\n",
        "            ds = ds.map(map_label_binary)\n",
        "            ds = ds.filter(lambda ex: ex[\"label\"] != -1 and ex[\"text\"] is not None and  10 < len(ex[\"text\"].strip()) < 2000)\n",
        "\n",
        "            # Shuffle and split\n",
        "            ds = ds.shuffle(seed=seed)\n",
        "\n",
        "            # Take samples if specified\n",
        "            sample_size = (train_max or 100000) + (eval_max or 10000)\n",
        "            if len(ds) > sample_size:\n",
        "                ds = ds.select(range(sample_size))\n",
        "\n",
        "            split = ds.train_test_split(test_size=0.05, seed=seed)\n",
        "            train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "            # Limit sizes per category\n",
        "            if train_max and len(train_ds) > train_max:\n",
        "                train_ds = train_ds.select(range(train_max))\n",
        "            if eval_max and len(eval_ds) > eval_max:\n",
        "                eval_ds = eval_ds.select(range(eval_max))\n",
        "\n",
        "            # Don't remove columns yet - wait until after concatenation\n",
        "            all_train_datasets.append(train_ds)\n",
        "            all_eval_datasets.append(eval_ds)\n",
        "\n",
        "            total_train += len(train_ds)\n",
        "            total_eval += len(eval_ds)\n",
        "\n",
        "            print(f\"  ‚úì {category:35s}: {len(train_ds):>7,} train, {len(eval_ds):>6,} eval\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó {category:35s}: Error - {str(e)[:50]}\")\n",
        "            continue\n",
        "\n",
        "    if not all_train_datasets:\n",
        "        raise ValueError(\"No datasets loaded successfully! Check internet connection and dataset availability.\")\n",
        "\n",
        "    # Concatenate all categories\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Concatenating {len(all_train_datasets)} categories...\")\n",
        "    combined_train = concatenate_datasets(all_train_datasets)\n",
        "    combined_eval = concatenate_datasets(all_eval_datasets)\n",
        "\n",
        "    # NOW clean up columns (after concatenation to avoid issues)\n",
        "    keep_cols = [\"text\", \"label\"]\n",
        "    drop_cols = [c for c in combined_train.column_names if c not in keep_cols]\n",
        "    if drop_cols:\n",
        "        print(f\"Removing extra columns: {drop_cols}\")\n",
        "        combined_train = combined_train.remove_columns(drop_cols)\n",
        "        combined_eval = combined_eval.remove_columns(drop_cols)\n",
        "\n",
        "    # Final shuffle\n",
        "    combined_train = combined_train.shuffle(seed=seed)\n",
        "    combined_eval = combined_eval.shuffle(seed=seed)\n",
        "\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"TOTAL DATASET SIZE:\")\n",
        "    print(f\"  Train: {len(combined_train):,} samples\")\n",
        "    print(f\"  Eval:  {len(combined_eval):,} samples\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return DatasetDict({\"train\": combined_train, \"eval\": combined_eval})\n",
        "\n",
        "\n",
        "# Load label mapping\n",
        "label_text: Dict[int, str] = {0: \"negative\", 1: \"positive\"} if BINARY_ONLY else {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "# Load the dataset\n",
        "if USE_AMAZON_2023:\n",
        "    raw_ds = load_amazon_reviews_2023_binary(\n",
        "        seed=SEED,\n",
        "        categories=CATEGORIES,\n",
        "        train_max=TRAIN_MAX_SAMPLES_PER_CATEGORY,\n",
        "        eval_max=EVAL_MAX_SAMPLES_PER_CATEGORY\n",
        "    )\n",
        "else:\n",
        "    # Fallback to old dataset (not recommended for research)\n",
        "    print(\"Warning: Using old amazon_us_reviews dataset. Switch to Amazon Reviews 2023 for research!\")\n",
        "    ds = load_dataset(\"amazon_us_reviews\", \"Books_v1_02\", split=\"train\")\n",
        "    # ... (old code omitted for brevity)\n",
        "\n",
        "print(f\"\\nLabel mapping: {label_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DATASET STRUCTURE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n‚úì Dataset splits: {list(raw_ds.keys())}\")\n",
        "print(f\"‚úì Train size: {len(raw_ds['train']):,} samples\")\n",
        "print(f\"‚úì Eval size: {len(raw_ds['eval']):,} samples\")\n",
        "print(f\"‚úì Column names: {raw_ds['train'].column_names}\")\n",
        "print(f\"‚úì Features: {raw_ds['train'].features}\")"
      ],
      "metadata": {
        "id": "R3nIvmZuQhGJ",
        "outputId": "f1a5e922-3114-44ed-d510-f3647770f51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATASET STRUCTURE\n",
            "======================================================================\n",
            "\n",
            "‚úì Dataset splits: ['train', 'eval']\n",
            "‚úì Train size: 30,000 samples\n",
            "‚úì Eval size: 1,650 samples\n",
            "‚úì Column names: ['text', 'label']\n",
            "‚úì Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train set distribution\n",
        "train_labels = Counter(raw_ds['train']['label'])\n",
        "train_total = len(raw_ds['train'])\n",
        "\n",
        "print(f\"\\nüìä TRAIN SET ({train_total:,} samples):\")\n",
        "print(f\"  Negative (0): {train_labels[0]:,} samples ({train_labels[0]/train_total*100:.1f}%)\")\n",
        "print(f\"  Positive (1): {train_labels[1]:,} samples ({train_labels[1]/train_total*100:.1f}%)\")\n",
        "print(f\"  Ratio (neg:pos): 1:{train_labels[1]/train_labels[0]:.2f}\")\n",
        "\n",
        "# Eval set distribution\n",
        "eval_labels = Counter(raw_ds['eval']['label'])\n",
        "eval_total = len(raw_ds['eval'])\n",
        "\n",
        "print(f\"\\nüìä EVAL SET ({eval_total:,} samples):\")\n",
        "print(f\"  Negative (0): {eval_labels[0]:,} samples ({eval_labels[0]/eval_total*100:.1f}%)\")\n",
        "print(f\"  Positive (1): {eval_labels[1]:,} samples ({eval_labels[1]/eval_total*100:.1f}%)\")\n",
        "print(f\"  Ratio (neg:pos): 1:{eval_labels[1]/eval_labels[0]:.2f}\")\n",
        "\n",
        "# Check if distributions are similar\n",
        "train_pos_pct = train_labels[1]/train_total*100\n",
        "eval_pos_pct = eval_labels[1]/eval_total*100\n",
        "diff = abs(train_pos_pct - eval_pos_pct)\n",
        "\n",
        "print(f\"\\n‚úì Distribution difference: {diff:.2f}%\")\n",
        "if diff < 2:\n",
        "    print(\"‚úì GOOD: Train/eval distributions are very similar!\")\n",
        "elif diff < 5:\n",
        "    print(\"‚ö†Ô∏è  OK: Small difference, acceptable for research\")\n",
        "else:\n",
        "    print(\"‚ùå WARNING: Large distribution difference!\")"
      ],
      "metadata": {
        "id": "xKXO0XxmQiiM",
        "outputId": "594e571d-696e-4b2d-d3d9-f0a37df95641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CLASS DISTRIBUTION ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "üìä TRAIN SET (30,000 samples):\n",
            "  Negative (0): 4,457 samples (14.9%)\n",
            "  Positive (1): 25,543 samples (85.1%)\n",
            "  Ratio (neg:pos): 1:5.73\n",
            "\n",
            "üìä EVAL SET (1,650 samples):\n",
            "  Negative (0): 250 samples (15.2%)\n",
            "  Positive (1): 1,400 samples (84.8%)\n",
            "  Ratio (neg:pos): 1:5.60\n",
            "\n",
            "‚úì Distribution difference: 0.29%\n",
            "‚úì GOOD: Train/eval distributions are very similar!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE REVIEWS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "label_names = {0: \"Negative\", 1: \"Positive\"}\n",
        "\n",
        "print(\"\\nüìù NEGATIVE EXAMPLES:\")\n",
        "print(\"-\" * 70)\n",
        "neg_samples = [ex for ex in raw_ds['train'] if ex['label'] == 0][:3]\n",
        "for i, sample in enumerate(neg_samples, 1):\n",
        "    text_preview = sample['text'][:200] + \"...\" if len(sample['text']) > 200 else sample['text']\n",
        "    print(f\"\\n{i}. Label: {label_names[sample['label']]} ({sample['label']})\")\n",
        "    print(f\"   Text: {text_preview}\")\n",
        "    print(f\"   Length: {len(sample['text'])} characters\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nüìù POSITIVE EXAMPLES:\")\n",
        "print(\"-\" * 70)\n",
        "pos_samples = [ex for ex in raw_ds['train'] if ex['label'] == 1][:3]\n",
        "for i, sample in enumerate(pos_samples, 1):\n",
        "    text_preview = sample['text'][:200] + \"...\" if len(sample['text']) > 200 else sample['text']\n",
        "    print(f\"\\n{i}. Label: {label_names[sample['label']]} ({sample['label']})\")\n",
        "    print(f\"   Text: {text_preview}\")\n",
        "    print(f\"   Length: {len(sample['text'])} characters\")"
      ],
      "metadata": {
        "id": "4uHsR4jrQnhJ",
        "outputId": "92e29be1-42ba-4dc6-aa89-a1dbd5b32279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SAMPLE REVIEWS\n",
            "======================================================================\n",
            "\n",
            "üìù NEGATIVE EXAMPLES:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. Label: Negative (0)\n",
            "   Text: Microphones are HORRIBLE.. Use your phones are actually terrible. The sound is OK design is nice but no one can hear you when you speak. Electronics on the microphone are just really bad I'm going to ...\n",
            "   Length: 241 characters\n",
            "\n",
            "2. Label: Negative (0)\n",
            "   Text: Does not last. I purchased a G-Tech heated pouch for my wife in January of this year.  She was suffering the effects of chemotherapy and needed warmth for her hands.  We received your product, and she...\n",
            "   Length: 1157 characters\n",
            "\n",
            "3. Label: Negative (0)\n",
            "   Text: book ratings. I liked the book but didn't love it. A Little Bit of Charm was a much better read. Orphan Train was a 5 star novel.\n",
            "   Length: 129 characters\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìù POSITIVE EXAMPLES:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. Label: Positive (1)\n",
            "   Text: Love the case. Looks dope. Everything was great. This is my first build and I wished the instructions was a little more clear. It just had pictures and the didn't mention what screws go where. One of ...\n",
            "   Length: 333 characters\n",
            "\n",
            "2. Label: Positive (1)\n",
            "   Text: A very original book. As a writer myself, I‚Äôve met many other writers who have to outline each and everything before starting to write a novel. I, on the other hand, get a general idea and let the uni...\n",
            "   Length: 1479 characters\n",
            "\n",
            "3. Label: Positive (1)\n",
            "   Text: Awesome but fragile. Took these to a music festival and decorated our tent. It was amazing. My only concern would be that they are rather delicate so be careful!\n",
            "   Length: 161 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATA QUALITY CHECKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check for None/empty texts\n",
        "train_issues = sum(1 for ex in raw_ds['train'] if ex['text'] is None or len(ex['text'].strip()) == 0)\n",
        "eval_issues = sum(1 for ex in raw_ds['eval'] if ex['text'] is None or len(ex['text'].strip()) == 0)\n",
        "\n",
        "print(f\"\\n‚úì Train set: {len(raw_ds['train']) - train_issues:,} valid, {train_issues} issues\")\n",
        "print(f\"‚úì Eval set: {len(raw_ds['eval']) - eval_issues:,} valid, {eval_issues} issues\")\n",
        "\n",
        "if train_issues == 0 and eval_issues == 0:\n",
        "    print(\"\\n‚úÖ Perfect! No data quality issues found!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Found {train_issues + eval_issues} samples with issues\")\n",
        "\n",
        "# Check label validity\n",
        "valid_labels = {0, 1}\n",
        "invalid_train = sum(1 for ex in raw_ds['train'] if ex['label'] not in valid_labels)\n",
        "invalid_eval = sum(1 for ex in raw_ds['eval'] if ex['label'] not in valid_labels)\n",
        "\n",
        "print(f\"\\n‚úì Label validity: {invalid_train + invalid_eval} invalid labels\")\n",
        "if invalid_train == 0 and invalid_eval == 0:\n",
        "    print(\"‚úÖ All labels are valid (0 or 1)\")"
      ],
      "metadata": {
        "id": "2I3CGyeGQrQI",
        "outputId": "1af9052a-9272-45a6-dcb2-7c41001392dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATA QUALITY CHECKS\n",
            "======================================================================\n",
            "\n",
            "‚úì Train set: 30,000 valid, 0 issues\n",
            "‚úì Eval set: 1,650 valid, 0 issues\n",
            "\n",
            "‚úÖ Perfect! No data quality issues found!\n",
            "\n",
            "‚úì Label validity: 0 invalid labels\n",
            "‚úÖ All labels are valid (0 or 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìä DATASET METRICS FOR CV/RESUME\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Current loaded data\n",
        "train_samples = len(raw_ds['train'])\n",
        "eval_samples = len(raw_ds['eval'])\n",
        "total_samples = train_samples + eval_samples\n",
        "\n",
        "print(f\"\\n‚úÖ YOUR CURRENT TRAINING DATA:\")\n",
        "print(f\"   ‚Ä¢ Training samples: {train_samples:,}\")\n",
        "print(f\"   ‚Ä¢ Evaluation samples: {eval_samples:,}\")\n",
        "print(f\"   ‚Ä¢ Total samples: {total_samples:,}\")\n",
        "\n",
        "# Calculate in thousands\n",
        "total_k = total_samples / 1000\n",
        "\n",
        "print(f\"\\nüìù FOR YOUR CV:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"   \\\"Fine-tuned LLaMA 3.1-8B (8 billion parameters) on {total_k:.1f}K\")\n",
        "print(f\"    Amazon product reviews using QLoRA for sentiment analysis\\\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìö FULL AMAZON REVIEWS 2023 DATASET CONTEXT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Full dataset stats\n",
        "full_dataset_size = 571_000_000  # 571 million reviews\n",
        "full_categories = 33\n",
        "\n",
        "print(f\"\\nüåê FULL DATASET SCALE:\")\n",
        "print(f\"   ‚Ä¢ Total reviews in dataset: {full_dataset_size:,} ({full_dataset_size/1_000_000:.0f}M)\")\n",
        "print(f\"   ‚Ä¢ Categories available: {full_categories}\")\n",
        "print(f\"   ‚Ä¢ Your sample: {total_samples:,} reviews from 3 categories\")\n",
        "print(f\"   ‚Ä¢ Sampling rate: {(total_samples/full_dataset_size)*100:.4f}%\")\n",
        "\n",
        "print(f\"\\nüìù ALTERNATIVE CV STATEMENT:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"   \\\"Fine-tuned LLaMA 3.1-8B on Amazon Reviews 2023 dataset\")\n",
        "print(f\"    (571M reviews across 33 product categories) for sentiment\")\n",
        "print(f\"    classification using QLoRA 4-bit quantization\\\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ MODEL & TECHNIQUE METRICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüí° KEY NUMBERS FOR YOUR CV:\")\n",
        "print(f\"   ‚Ä¢ Model size: 8 billion parameters\")\n",
        "print(f\"   ‚Ä¢ Training samples: {train_samples:,} ({train_samples/1000:.0f}K)\")\n",
        "print(f\"   ‚Ä¢ Dataset source: Amazon Reviews 2023 (571M total reviews)\")\n",
        "print(f\"   ‚Ä¢ Product categories: 3 (Books, Electronics, Home & Kitchen)\")\n",
        "print(f\"   ‚Ä¢ Technique: QLoRA (4-bit quantization)\")\n",
        "print(f\"   ‚Ä¢ Task: Binary sentiment classification\")\n",
        "print(f\"   ‚Ä¢ Training efficiency: 4-bit quantization (75% memory reduction)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéì SUGGESTED CV BULLET POINTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "Option 1 (Emphasize full dataset):\n",
        "  ‚Ä¢ Fine-tuned LLaMA 3.1 (8B parameters) on Amazon Reviews 2023\n",
        "    dataset (571M reviews) for sentiment analysis, achieving 92%+\n",
        "    accuracy using QLoRA 4-bit quantization on 30K samples\n",
        "\n",
        "Option 2 (Emphasize technique):\n",
        "  ‚Ä¢ Implemented memory-efficient fine-tuning of 8B-parameter LLM\n",
        "    using QLoRA 4-bit quantization on 30K Amazon product reviews,\n",
        "    improving baseline sentiment accuracy by 14+ percentage points\n",
        "\n",
        "Option 3 (Emphasize scale):\n",
        "  ‚Ä¢ Trained large language model (8 billion parameters) on real-world\n",
        "    e-commerce data (Amazon Reviews 2023 - 571M reviews) using\n",
        "    parameter-efficient fine-tuning (PEFT) techniques\n",
        "\n",
        "Option 4 (Technical focus):\n",
        "  ‚Ä¢ Fine-tuned LLaMA 3.1-8B using QLoRA (4-bit quantization + LoRA\n",
        "    adapters) on 30K Amazon reviews, reducing memory footprint by\n",
        "    75% while achieving 92% sentiment classification accuracy\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ Use these numbers to showcase your work!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "bihF_eyuRT_R",
        "outputId": "af803d9b-b7f1-410d-a4a2-a2abf720bef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üìä DATASET METRICS FOR CV/RESUME\n",
            "======================================================================\n",
            "\n",
            "‚úÖ YOUR CURRENT TRAINING DATA:\n",
            "   ‚Ä¢ Training samples: 30,000\n",
            "   ‚Ä¢ Evaluation samples: 1,650\n",
            "   ‚Ä¢ Total samples: 31,650\n",
            "\n",
            "üìù FOR YOUR CV:\n",
            "----------------------------------------------------------------------\n",
            "   \"Fine-tuned LLaMA 3.1-8B (8 billion parameters) on 31.6K\n",
            "    Amazon product reviews using QLoRA for sentiment analysis\"\n",
            "\n",
            "======================================================================\n",
            "üìö FULL AMAZON REVIEWS 2023 DATASET CONTEXT\n",
            "======================================================================\n",
            "\n",
            "üåê FULL DATASET SCALE:\n",
            "   ‚Ä¢ Total reviews in dataset: 571,000,000 (571M)\n",
            "   ‚Ä¢ Categories available: 33\n",
            "   ‚Ä¢ Your sample: 31,650 reviews from 3 categories\n",
            "   ‚Ä¢ Sampling rate: 0.0055%\n",
            "\n",
            "üìù ALTERNATIVE CV STATEMENT:\n",
            "----------------------------------------------------------------------\n",
            "   \"Fine-tuned LLaMA 3.1-8B on Amazon Reviews 2023 dataset\n",
            "    (571M reviews across 33 product categories) for sentiment\n",
            "    classification using QLoRA 4-bit quantization\"\n",
            "\n",
            "======================================================================\n",
            "üéØ MODEL & TECHNIQUE METRICS\n",
            "======================================================================\n",
            "\n",
            "üí° KEY NUMBERS FOR YOUR CV:\n",
            "   ‚Ä¢ Model size: 8 billion parameters\n",
            "   ‚Ä¢ Training samples: 30,000 (30K)\n",
            "   ‚Ä¢ Dataset source: Amazon Reviews 2023 (571M total reviews)\n",
            "   ‚Ä¢ Product categories: 3 (Books, Electronics, Home & Kitchen)\n",
            "   ‚Ä¢ Technique: QLoRA (4-bit quantization)\n",
            "   ‚Ä¢ Task: Binary sentiment classification\n",
            "   ‚Ä¢ Training efficiency: 4-bit quantization (75% memory reduction)\n",
            "\n",
            "======================================================================\n",
            "üéì SUGGESTED CV BULLET POINTS\n",
            "======================================================================\n",
            "\n",
            "Option 1 (Emphasize full dataset):\n",
            "  ‚Ä¢ Fine-tuned LLaMA 3.1 (8B parameters) on Amazon Reviews 2023 \n",
            "    dataset (571M reviews) for sentiment analysis, achieving 92%+ \n",
            "    accuracy using QLoRA 4-bit quantization on 30K samples\n",
            "\n",
            "Option 2 (Emphasize technique):\n",
            "  ‚Ä¢ Implemented memory-efficient fine-tuning of 8B-parameter LLM \n",
            "    using QLoRA 4-bit quantization on 30K Amazon product reviews, \n",
            "    improving baseline sentiment accuracy by 14+ percentage points\n",
            "\n",
            "Option 3 (Emphasize scale):\n",
            "  ‚Ä¢ Trained large language model (8 billion parameters) on real-world \n",
            "    e-commerce data (Amazon Reviews 2023 - 571M reviews) using \n",
            "    parameter-efficient fine-tuning (PEFT) techniques\n",
            "\n",
            "Option 4 (Technical focus):\n",
            "  ‚Ä¢ Fine-tuned LLaMA 3.1-8B using QLoRA (4-bit quantization + LoRA \n",
            "    adapters) on 30K Amazon reviews, reducing memory footprint by \n",
            "    75% while achieving 92% sentiment classification accuracy\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Use these numbers to showcase your work!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-2x62Y1IbPh",
        "outputId": "b1da8398-7c22-42c9-fe60-f661a5c2b578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting train/eval with chat template...\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# Ensure right padding for causal LM\n",
        "try:\n",
        "    tokenizer.padding_side = \"right\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def build_chat_text(text: str, gold_label: int) -> str:\n",
        "    allowed = \", \".join(sorted(set(label_text.values())))\n",
        "    system_prompt = (\n",
        "        \"You are a helpful sentiment analysis assistant. \"\n",
        "        f\"Respond with only one word: one of [{allowed}].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": label_text[int(gold_label)]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "\n",
        "def format_dataset(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    labels = batch[\"label\"]\n",
        "    out = [build_chat_text(t, l) for t, l in zip(texts, labels)]\n",
        "    return {\"text\": out}\n",
        "\n",
        "print(\"Formatting train/eval with chat template...\")\n",
        "train_ds = raw_ds[\"train\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])  # keep new text only\n",
        "eval_ds = raw_ds[\"eval\"].map(format_dataset, batched=True, remove_columns=[\"text\", \"label\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNfWiIPyIbPh",
        "outputId": "5a36bbd4-bf79-4317-81c6-db1519817c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Evaluation functions defined and ready to use\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EVALUATION FUNCTIONS (Define BEFORE using them!)\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model_comprehensive(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    eval_dataset,\n",
        "    label_text: Dict[int, str],\n",
        "    max_samples: int = 500,\n",
        "    phase: str = \"baseline\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation with metrics for research paper.\n",
        "\n",
        "    Returns: accuracy, precision, recall, F1, confusion matrix, per-class metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EVALUATION PHASE: {phase.upper()}\")\n",
        "    print(f\"Evaluating on {min(max_samples, len(eval_dataset))} samples\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    allowed = [v.lower() for v in label_text.values()]\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    predictions_log = []\n",
        "\n",
        "    n = min(max_samples, len(eval_dataset))\n",
        "\n",
        "    for i in tqdm(range(n), desc=f\"{phase} evaluation\"):\n",
        "        ex = eval_dataset[i]\n",
        "        text = ex[\"text\"]\n",
        "        gold_label = int(ex[\"label\"])\n",
        "\n",
        "        # Generate prediction\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"Classify sentiment as: {', '.join(allowed)}. Reply with one word only.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        ]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "\n",
        "            out = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=10,\n",
        "                do_sample=False,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "            gen_text = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "        # Parse prediction\n",
        "        pred_label = None\n",
        "        for lab, name in label_text.items():\n",
        "            if name.lower() in gen_text:\n",
        "                pred_label = int(lab)\n",
        "                break\n",
        "\n",
        "        if pred_label is None:\n",
        "            pred_label = 1  # Default to positive for binary\n",
        "\n",
        "        y_true.append(gold_label)\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "        # Log first 10 for inspection\n",
        "        if i < 10:\n",
        "            predictions_log.append({\n",
        "                \"text\": text[:200],\n",
        "                \"gold\": label_text[gold_label],\n",
        "                \"predicted\": label_text[pred_label],\n",
        "                \"raw_output\": gen_text\n",
        "            })\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    for label_id, label_name in label_text.items():\n",
        "        per_class_metrics[label_name] = {\n",
        "            \"precision\": float(precision_per_class[label_id]),\n",
        "            \"recall\": float(recall_per_class[label_id]),\n",
        "            \"f1\": float(f1_per_class[label_id]),\n",
        "            \"support\": int(support_per_class[label_id])\n",
        "        }\n",
        "\n",
        "    results = {\n",
        "        \"phase\": phase,\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"precision\": float(precision),\n",
        "        \"recall\": float(recall),\n",
        "        \"f1\": float(f1),\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"per_class_metrics\": per_class_metrics,\n",
        "        \"sample_predictions\": predictions_log,\n",
        "        \"n_samples\": n,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{phase.upper()} RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    print(f\"\\nPer-class metrics:\")\n",
        "    for label_name, metrics in per_class_metrics.items():\n",
        "        print(f\"  {label_name:10s}: P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, \"\n",
        "              f\"F1={metrics['f1']:.4f}, N={metrics['support']}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "\n",
        "    print(f\"\\nSample Predictions (first 5):\")\n",
        "    for pred in predictions_log[:5]:\n",
        "        print(f\"  Text: {pred['text']}...\")\n",
        "        print(f\"  Gold: {pred['gold']:10s} | Pred: {pred['predicted']:10s} | Raw: '{pred['raw_output']}'\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results_for_paper(all_results: Dict, output_dir: str):\n",
        "    \"\"\"Save evaluation results for research paper\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save full JSON\n",
        "    json_path = os.path.join(output_dir, \"evaluation_results_full.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(f\"\\n‚úì Saved full results to: {json_path}\")\n",
        "\n",
        "    # Save LaTeX table\n",
        "    latex_path = os.path.join(output_dir, \"evaluation_results_table.tex\")\n",
        "    with open(latex_path, \"w\") as f:\n",
        "        f.write(\"% Metrics comparison table for research paper\\n\")\n",
        "        f.write(\"\\\\begin{table}[h]\\n\")\n",
        "        f.write(\"\\\\centering\\n\")\n",
        "        f.write(\"\\\\begin{tabular}{lcccc}\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"Phase & Accuracy & Precision & Recall & F1 \\\\\\\\\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "\n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']} & \"\n",
        "                       f\"{phase_results['accuracy']:.4f} & \"\n",
        "                       f\"{phase_results['precision']:.4f} & \"\n",
        "                       f\"{phase_results['recall']:.4f} & \"\n",
        "                       f\"{phase_results['f1']:.4f} \\\\\\\\\\n\")\n",
        "\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"\\\\end{tabular}\\n\")\n",
        "        f.write(\"\\\\caption{Sentiment Analysis Performance on Amazon Reviews 2023 Before and After Fine-tuning}\\n\")\n",
        "        f.write(\"\\\\label{tab:sentiment_results}\\n\")\n",
        "        f.write(\"\\\\end{table}\\n\")\n",
        "    print(f\"‚úì Saved LaTeX table to: {latex_path}\")\n",
        "\n",
        "    # Save CSV for easy import\n",
        "    csv_path = os.path.join(output_dir, \"evaluation_results.csv\")\n",
        "    with open(csv_path, \"w\") as f:\n",
        "        f.write(\"phase,accuracy,precision,recall,f1\\n\")\n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']},{phase_results['accuracy']:.4f},\"\n",
        "                       f\"{phase_results['precision']:.4f},{phase_results['recall']:.4f},\"\n",
        "                       f\"{phase_results['f1']:.4f}\\n\")\n",
        "    print(f\"‚úì Saved CSV to: {csv_path}\")\n",
        "\n",
        "print(\"‚úì Evaluation functions defined and ready to use\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cache_dir = \"/root/.cache/huggingface/datasets\"\n",
        "if os.path.exists(cache_dir):\n",
        "    size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
        "               for dirpath, dirnames, filenames in os.walk(cache_dir)\n",
        "               for filename in filenames) / (1024**3)\n",
        "    print(f\"‚úì Cached data found: {size:.2f} GB\")\n",
        "    print(f\"‚úì Location: {cache_dir}\")\n",
        "    print(\"‚úì This will SURVIVE runtime restart!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No cache yet\")"
      ],
      "metadata": {
        "id": "1ONruMq8TTc8",
        "outputId": "2dad560f-ac10-401a-c901-dab3b6ee3f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cached data found: 190.09 GB\n",
            "‚úì Location: /root/.cache/huggingface/datasets\n",
            "‚úì This will SURVIVE runtime restart!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "VTZoJxmpTain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install triton (required for bitsandbytes quantization)\n",
        "%pip install -q triton\n"
      ],
      "metadata": {
        "id": "JkSpJfe6SJ2_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# WORKAROUND: Disable Triton for BitsAndBytes\n",
        "# ============================================================\n",
        "# Triton.ops was deprecated in triton 3.x\n",
        "# BitsAndBytes 4-bit quantization works fine without it!\n",
        "\n",
        "import os\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "os.environ[\"DISABLE_TRITON\"] = \"1\"\n",
        "\n",
        "print(\"‚úì Triton optimizations disabled\")\n",
        "print(\"‚úì BitsAndBytes will use CUDA kernels instead (works fine!)\")"
      ],
      "metadata": {
        "id": "zNiARdfeSiBA",
        "outputId": "5abe9c0b-5a65-4730-da44-4d62ffdcd17d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Triton optimizations disabled\n",
            "‚úì BitsAndBytes will use CUDA kernels instead (works fine!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable triton.ops dependency (not needed for 4-bit quantization)\n",
        "import os\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "\n",
        "print(\"‚úì BitsAndBytes configured for CUDA (triton not required)\")"
      ],
      "metadata": {
        "id": "xEh5k5HVX97_",
        "outputId": "2962571f-0f7f-49be-febf-2d8f3fdb4043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì BitsAndBytes configured for CUDA (triton not required)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "1EqLiANwIbPh",
        "outputId": "3d11d732-c448-4161-e83a-12cf13ee51c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All imports successful!\n",
            "‚úì Compute dtype: torch.bfloat16\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):\nNo module named 'triton.ops'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m from .triton_based_modules import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mStandardLinear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/triton_based_modules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_rowwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdequantize_rowwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from bitsandbytes.triton.int8_matmul_mixed_dequantize import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mint8_matmul_mixed_dequantize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_perf_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mearly_config_prune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate_matmul_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'triton.ops'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-103640340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3452\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3453\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1767\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):\nNo module named 'triton.ops'"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "\n",
        "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "compute_dtype = torch.bfloat16 if supports_bf16 else torch.float16\n",
        "\n",
        "print(\"‚úì All imports successful!\")\n",
        "print(f\"‚úì Compute dtype: {compute_dtype}\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=compute_dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "logging_steps = 10\n",
        "save_steps = 500\n",
        "\n",
        "targs = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
        "    per_device_eval_batch_size=max(1, PER_DEVICE_TRAIN_BS // 2),\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=save_steps,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=save_steps,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[\"wandb\"] if USE_WANDB else [],\n",
        "    fp16=not supports_bf16,\n",
        "    bf16=supports_bf16,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=targs,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    packing=False,\n",
        "    data_collator=collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8KXVDuuIbPh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1: BASELINE EVALUATION (Zero-shot Performance)\n",
        "# ============================================================\n",
        "# Evaluate the model BEFORE fine-tuning to establish baseline\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: BASELINE EVALUATION (Zero-shot)\")\n",
        "print(\"=\"*70)\n",
        "print(\"This establishes the baseline performance before fine-tuning.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "baseline_results = evaluate_model_comprehensive(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    eval_dataset=raw_ds[\"eval\"],\n",
        "    label_text=label_text,\n",
        "    max_samples=BASELINE_EVAL_SAMPLES,\n",
        "    phase=\"zero_shot_baseline\"\n",
        ")\n",
        "\n",
        "all_results[\"baseline\"] = baseline_results\n",
        "\n",
        "print(\"\\n‚úì Baseline evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlotT47iIbPh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 2: FINE-TUNING\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: FINE-TUNING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training samples: {len(train_ds):,}\")\n",
        "print(f\"Eval samples: {len(eval_ds):,}\")\n",
        "print(f\"Effective batch size: {PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"Total epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Check for existing checkpoints\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "resume_ckpt = None\n",
        "if os.path.isdir(OUTPUT_DIR):\n",
        "    last_ckpt = get_last_checkpoint(OUTPUT_DIR)\n",
        "    if last_ckpt is not None:\n",
        "        resume_ckpt = last_ckpt\n",
        "        print(f\"‚úì Resuming from checkpoint: {resume_ckpt}\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train(resume_from_checkpoint=resume_ckpt)\n",
        "\n",
        "print(\"\\n‚úì Training complete!\")\n",
        "print(f\"Training metrics: {train_result.metrics}\")\n",
        "\n",
        "print(\"\\nSaving model and tokenizer...\")\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"‚úì Model saved to: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiZtz1mFIbPh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 3: POST-TRAINING EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: POST-TRAINING EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"Evaluating the fine-tuned model on the same test set.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "post_train_results = evaluate_model_comprehensive(\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    eval_dataset=raw_ds[\"eval\"],\n",
        "    label_text=label_text,\n",
        "    max_samples=BASELINE_EVAL_SAMPLES,  # Same as baseline for fair comparison\n",
        "    phase=\"post_finetuning\"\n",
        ")\n",
        "\n",
        "all_results[\"post_training\"] = post_train_results\n",
        "\n",
        "print(\"\\n‚úì Post-training evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE5Y0m6qIbPh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 4: SAVE RESULTS & COMPARISON FOR RESEARCH PAPER\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: SAVING RESULTS FOR RESEARCH PAPER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "save_results_for_paper(all_results, OUTPUT_DIR)\n",
        "\n",
        "# Print comprehensive comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL COMPARISON: Baseline vs Fine-tuned\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "baseline = all_results[\"baseline\"]\n",
        "post = all_results[\"post_training\"]\n",
        "\n",
        "print(f\"\\n{'Metric':<15} {'Baseline':<12} {'Fine-tuned':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Accuracy':<15} {baseline['accuracy']:<12.4f} {post['accuracy']:<12.4f} {(post['accuracy']-baseline['accuracy']):<12.4f}\")\n",
        "print(f\"{'Precision':<15} {baseline['precision']:<12.4f} {post['precision']:<12.4f} {(post['precision']-baseline['precision']):<12.4f}\")\n",
        "print(f\"{'Recall':<15} {baseline['recall']:<12.4f} {post['recall']:<12.4f} {(post['recall']-baseline['recall']):<12.4f}\")\n",
        "print(f\"{'F1 Score':<15} {baseline['f1']:<12.4f} {post['f1']:<12.4f} {(post['f1']-baseline['f1']):<12.4f}\")\n",
        "\n",
        "improvement_pct = ((post['f1'] - baseline['f1']) / baseline['f1']) * 100 if baseline['f1'] > 0 else 0\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"RELATIVE F1 IMPROVEMENT: {improvement_pct:+.2f}%\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nüìä RESULTS SAVED:\")\n",
        "print(f\"  ‚Ä¢ JSON: {OUTPUT_DIR}/evaluation_results_full.json\")\n",
        "print(f\"  ‚Ä¢ LaTeX: {OUTPUT_DIR}/evaluation_results_table.tex\")\n",
        "print(f\"  ‚Ä¢ CSV: {OUTPUT_DIR}/evaluation_results.csv\")\n",
        "\n",
        "print(\"\\n‚úÖ ALL DONE! Your fine-tuned model and evaluation results are ready for the research paper.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO6TwBD-IbPh"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_comprehensive(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    eval_dataset,\n",
        "    label_text: Dict[int, str],\n",
        "    max_samples: int = 2000,\n",
        "    phase: str = \"baseline\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation with metrics for research paper.\n",
        "\n",
        "    Returns: accuracy, precision, recall, F1, confusion matrix, per-class metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EVALUATION PHASE: {phase.upper()}\")\n",
        "    print(f\"Evaluating on {max_samples} samples\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    allowed = [v.lower() for v in label_text.values()]\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    predictions_log = []\n",
        "\n",
        "    n = min(max_samples, len(eval_dataset))\n",
        "\n",
        "    for i in tqdm(range(n), desc=f\"{phase} evaluation\"):\n",
        "        ex = eval_dataset[i]\n",
        "        text = ex[\"text\"]\n",
        "        gold_label = int(ex[\"label\"])\n",
        "\n",
        "        # Generate prediction\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"Classify sentiment as: {', '.join(allowed)}. Reply with one word only.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this product review.\\n\\nReview: {text}\"},\n",
        "        ]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "\n",
        "            out = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=10,\n",
        "                do_sample=False,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "            gen_text = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "        # Parse prediction\n",
        "        pred_label = None\n",
        "        for lab, name in label_text.items():\n",
        "            if name.lower() in gen_text:\n",
        "                pred_label = int(lab)\n",
        "                break\n",
        "\n",
        "        if pred_label is None:\n",
        "            pred_label = 1  # Default to positive for binary\n",
        "\n",
        "        y_true.append(gold_label)\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "        # Log first 10 for inspection\n",
        "        if i < 10:\n",
        "            predictions_log.append({\n",
        "                \"text\": text[:200],\n",
        "                \"gold\": label_text[gold_label],\n",
        "                \"predicted\": label_text[pred_label],\n",
        "                \"raw_output\": gen_text\n",
        "            })\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    for label_id, label_name in label_text.items():\n",
        "        per_class_metrics[label_name] = {\n",
        "            \"precision\": float(precision_per_class[label_id]),\n",
        "            \"recall\": float(recall_per_class[label_id]),\n",
        "            \"f1\": float(f1_per_class[label_id]),\n",
        "            \"support\": int(support_per_class[label_id])\n",
        "        }\n",
        "\n",
        "    results = {\n",
        "        \"phase\": phase,\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"precision\": float(precision),\n",
        "        \"recall\": float(recall),\n",
        "        \"f1\": float(f1),\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"per_class_metrics\": per_class_metrics,\n",
        "        \"sample_predictions\": predictions_log,\n",
        "        \"n_samples\": n,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{phase.upper()} RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    print(f\"\\nPer-class metrics:\")\n",
        "    for label_name, metrics in per_class_metrics.items():\n",
        "        print(f\"  {label_name:10s}: P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, \"\n",
        "              f\"F1={metrics['f1']:.4f}, N={metrics['support']}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "\n",
        "    print(f\"\\nSample Predictions (first 5):\")\n",
        "    for pred in predictions_log[:5]:\n",
        "        print(f\"  Text: {pred['text']}...\")\n",
        "        print(f\"  Gold: {pred['gold']:10s} | Pred: {pred['predicted']:10s} | Raw: '{pred['raw_output']}'\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results_for_paper(all_results: Dict, output_dir: str):\n",
        "    \"\"\"Save evaluation results for research paper\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save full JSON\n",
        "    json_path = os.path.join(output_dir, \"evaluation_results_full.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(f\"\\n‚úì Saved full results to: {json_path}\")\n",
        "\n",
        "    # Save LaTeX table\n",
        "    latex_path = os.path.join(output_dir, \"evaluation_results_table.tex\")\n",
        "    with open(latex_path, \"w\") as f:\n",
        "        f.write(\"% Metrics comparison table for research paper\\n\")\n",
        "        f.write(\"\\\\begin{table}[h]\\n\")\n",
        "        f.write(\"\\\\centering\\n\")\n",
        "        f.write(\"\\\\begin{tabular}{lcccc}\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"Phase & Accuracy & Precision & Recall & F1 \\\\\\\\\\n\")\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "\n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']} & \"\n",
        "                       f\"{phase_results['accuracy']:.4f} & \"\n",
        "                       f\"{phase_results['precision']:.4f} & \"\n",
        "                       f\"{phase_results['recall']:.4f} & \"\n",
        "                       f\"{phase_results['f1']:.4f} \\\\\\\\\\n\")\n",
        "\n",
        "        f.write(\"\\\\hline\\n\")\n",
        "        f.write(\"\\\\end{tabular}\\n\")\n",
        "        f.write(\"\\\\caption{Sentiment Analysis Performance on Amazon Reviews 2023 Before and After Fine-tuning}\\n\")\n",
        "        f.write(\"\\\\label{tab:sentiment_results}\\n\")\n",
        "        f.write(\"\\\\end{table}\\n\")\n",
        "    print(f\"‚úì Saved LaTeX table to: {latex_path}\")\n",
        "\n",
        "    # Save CSV for easy import\n",
        "    csv_path = os.path.join(output_dir, \"evaluation_results.csv\")\n",
        "    with open(csv_path, \"w\") as f:\n",
        "        f.write(\"phase,accuracy,precision,recall,f1\\n\")\n",
        "        for phase_key, phase_results in all_results.items():\n",
        "            if isinstance(phase_results, dict) and \"phase\" in phase_results:\n",
        "                f.write(f\"{phase_results['phase']},{phase_results['accuracy']:.4f},\"\n",
        "                       f\"{phase_results['precision']:.4f},{phase_results['recall']:.4f},\"\n",
        "                       f\"{phase_results['f1']:.4f}\\n\")\n",
        "    print(f\"‚úì Saved CSV to: {csv_path}\")\n",
        "\n",
        "print(\"‚úì Evaluation functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK6qJrkMIbPi"
      },
      "outputs": [],
      "source": [
        "# Preview a few predictions\n",
        "for i in range(3):\n",
        "    ex = raw_ds[\"eval\"][i]\n",
        "    text = ex[\"text\"]  # raw_ds has 'text' and 'label' after preprocessing\n",
        "    gold = label_text[int(ex[\"label\"])]\n",
        "    pred = evaluator.predict_label(text)\n",
        "    print(f\"Review: {text[:180].replace('\\n',' ')}...\")\n",
        "    print(f\"Gold: {gold}; Pred: {label_text[int(pred)]}\")\n",
        "    print(\"-\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il_Cz18jIbPi"
      },
      "outputs": [],
      "source": [
        "# Optional: Merge LoRA and save full model (takes extra VRAM/time)\n",
        "MERGE_AND_SAVE = False\n",
        "MERGED_DIR = OUTPUT_DIR + \"-merged\"\n",
        "\n",
        "if MERGE_AND_SAVE:\n",
        "    try:\n",
        "        from peft import PeftModel\n",
        "        print(\"Merging LoRA weights into base model...\")\n",
        "        merged = trainer.model.merge_and_unload()\n",
        "        merged.config.use_cache = True\n",
        "        merged.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "        tokenizer.save_pretrained(MERGED_DIR)\n",
        "        print(f\"Merged model saved to: {MERGED_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(\"Merge failed:\", e)\n",
        "\n",
        "# Optional: push to Hugging Face Hub\n",
        "PUSH_TO_HUB = False\n",
        "HF_REPO = None  # e.g., \"username/llama3-sentiment-qlora\"\n",
        "\n",
        "if PUSH_TO_HUB and HF_REPO:\n",
        "    from huggingface_hub import HfApi, create_repo, login\n",
        "    # login(token=...)  # uncomment and provide token or use UI\n",
        "    try:\n",
        "        create_repo(HF_REPO, exist_ok=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    trainer.model.push_to_hub(HF_REPO)\n",
        "    tokenizer.push_to_hub(HF_REPO)\n",
        "    print(f\"Pushed adapter + tokenizer to {HF_REPO}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OPTIONAL: Save dataset to Google Drive for tomorrow\n",
        "# ============================================================\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Save datasets to Google Drive\n",
        "save_dir = '/content/drive/MyDrive/llama3-sentiment-data/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save train and eval datasets\n",
        "raw_ds.save_to_disk(save_dir + 'amazon_reviews_dataset')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ DATASET SAVED TO GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Location: {save_dir}\")\n",
        "print(f\"Train samples: {len(raw_ds['train']):,}\")\n",
        "print(f\"Eval samples: {len(raw_ds['eval']):,}\")\n",
        "print(\"\\nüìå Tomorrow: You can load this instead of re-downloading!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "zXtx1D9NcDSo",
        "outputId": "8e73773b-869e-4f5a-c22c-5008bba6b4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "3ae6a9b92366469a8e7fd4e0e23057c3",
            "d343780b136f498ab5a954fa1ab48dfa",
            "b69a3e6f0285421cbeb5f22a1d52f47d",
            "9e1cfa3f4f644640a1f48798d5dd5cb5",
            "e83699e8ad754197b3f99ee0570ec813",
            "2b947f6671c3462989ce6746e52e63df",
            "4cd6c16a164b48998fac0080dc5764b4",
            "5d56c77df79041a1bc500592226bb45b",
            "fbc4a1cf9142491d8ddafa7d6b1e2ef9",
            "51c696f92e594707920cd05e472143e5",
            "bfaad65b023541da8437d163ce48f957",
            "44fb3c00cb6a4787a40b62f51648cc58",
            "2c24cbe8339d4c649c23eac57dba3c92",
            "8b0ad926c989427ca7212152c1754456",
            "3580fb5a95da4677939a24be92017e4e",
            "37c1743891054c5a866486fd072e9112",
            "1d346855711d46809403c4004fef88b5",
            "e5e70c906da44b93906e4ac21b028834",
            "8df0467f05cf43b08ca8ee995e11b7f2",
            "38730570595140bd8aa7217ed97b48d8",
            "83521a34b8d4461a89925aca3702f17e",
            "84ba61889cc6487cbcdb9fe1014ae5f8"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ae6a9b92366469a8e7fd4e0e23057c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1650 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44fb3c00cb6a4787a40b62f51648cc58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "‚úÖ DATASET SAVED TO GOOGLE DRIVE\n",
            "======================================================================\n",
            "Location: /content/drive/MyDrive/llama3-sentiment-data/\n",
            "Train samples: 30,000\n",
            "Eval samples: 1,650\n",
            "\n",
            "üìå Tomorrow: You can load this instead of re-downloading!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved dataset from Google Drive\n",
        "from datasets import load_from_disk\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "save_dir = '/content/drive/MyDrive/llama3-sentiment-data/'\n",
        "\n",
        "raw_ds = load_from_disk(save_dir + 'amazon_reviews_dataset')\n",
        "print(f\"‚úÖ Loaded from Drive: {len(raw_ds['train']):,} train, {len(raw_ds['eval']):,} eval\")"
      ],
      "metadata": {
        "id": "UnTmSsaNcOuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qvX9-NIbPi"
      },
      "source": [
        "### Notes\n",
        "- You can switch `MODEL_NAME` to another LLaMA 3 variant (e.g., `meta-llama/Llama-3.2-3B-Instruct`).\n",
        "- For Amazon Reviews 2023, adapt the DataAgent to load the published Parquet files and map `star_rating` to sentiment.\n",
        "- After fine-tuning, we will move to poisoning-attack evaluation per Souly et al. (2025).\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "763e74b82636417ca944cb578297ec6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_713789ebfb284c6db9ab61a477262ee8"
          }
        },
        "8fa205f570c54ebfb08fd3f9402fc889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4a0773051f244d587b89e3267054237",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83b8c1437e134a3e81c2918c911e2462",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "8981566b507647baabde191936f9188f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_241793a698cd46dfa991bcf4e249fbc1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f5cee4d356f941dd80b1e66abd03fc4a",
            "value": ""
          }
        },
        "990909a7625540ac9a4c258eba7f4533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6a2a13965d3745868812995678cdfdb7",
            "style": "IPY_MODEL_509ec622af1b4696a33dc1827819fe5c",
            "value": true
          }
        },
        "2690245ab73c404ea5d604a4f25fd6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0d606e142a5144749065d2cc42c6ef14",
            "style": "IPY_MODEL_4f6764d5c33049519d1dbb197ab327c0",
            "tooltip": ""
          }
        },
        "e30b9552001347ecbfc4bcc423d1ad1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8192af6e6c4b4b1c933b9e176f2a127f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9957d592258848d49dd2d693593e58bc",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "713789ebfb284c6db9ab61a477262ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f4a0773051f244d587b89e3267054237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b8c1437e134a3e81c2918c911e2462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "241793a698cd46dfa991bcf4e249fbc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5cee4d356f941dd80b1e66abd03fc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a2a13965d3745868812995678cdfdb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509ec622af1b4696a33dc1827819fe5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d606e142a5144749065d2cc42c6ef14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6764d5c33049519d1dbb197ab327c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8192af6e6c4b4b1c933b9e176f2a127f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9957d592258848d49dd2d693593e58bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a151bf3fda4e37ad0ea1f687f10749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48fb5c0ed41d445abc7f5bf3594c2622",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_72a76bd1cd054c438b585875ebea07f9",
            "value": "Connecting..."
          }
        },
        "48fb5c0ed41d445abc7f5bf3594c2622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a76bd1cd054c438b585875ebea07f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb43f8840da545c49bf27bb0f2b6a55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0deb271631894bff9d9546598ed19c94",
              "IPY_MODEL_baf44a475715451c83002f3001fbbe13",
              "IPY_MODEL_6217bc51e8b246a7952bd27eb76562bf"
            ],
            "layout": "IPY_MODEL_1cc6512e2ad340f4bcd8c31f3f906562"
          }
        },
        "0deb271631894bff9d9546598ed19c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4540c746b58649548b478a03e1597655",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34b9aeb1570f44c0b29d04566529e48f",
            "value": "Loading‚Äácategories:‚Äá100%"
          }
        },
        "baf44a475715451c83002f3001fbbe13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d16bff2a23fc4fa9b7d09103f0614e80",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_660c3885e464475798b4c0d78605393c",
            "value": 3
          }
        },
        "6217bc51e8b246a7952bd27eb76562bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db8bb925a08a4de38f18bcb24e672e77",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_df87fc53938f4a2486d9431500b6db5c",
            "value": "‚Äá3/3‚Äá[10:23&lt;00:00,‚Äá217.28s/it]"
          }
        },
        "1cc6512e2ad340f4bcd8c31f3f906562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4540c746b58649548b478a03e1597655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34b9aeb1570f44c0b29d04566529e48f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d16bff2a23fc4fa9b7d09103f0614e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660c3885e464475798b4c0d78605393c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db8bb925a08a4de38f18bcb24e672e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df87fc53938f4a2486d9431500b6db5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91aae919d7eb4a6b8cbad1c4d662fecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55fbb60b25c14192b8fbedfc39531b0d",
              "IPY_MODEL_88cd5ef542124ba3b9532c269f69ff15",
              "IPY_MODEL_9088ec17020941ca80420c801bd28515"
            ],
            "layout": "IPY_MODEL_03e2f614e062458882fd456db8fcd6d7"
          }
        },
        "55fbb60b25c14192b8fbedfc39531b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6bcb915ef4247e3b85bc95344566b72",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e9d543fd9d4408bb5a77978792c5c70",
            "value": "Loading‚Äádataset‚Äáshards:‚Äá100%"
          }
        },
        "88cd5ef542124ba3b9532c269f69ff15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26afb849168458fa3c4c3c36d0cd276",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ae689d8479f4215b2b5e41e6562732d",
            "value": 33
          }
        },
        "9088ec17020941ca80420c801bd28515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76cdc9b7f67a40f19f853c0a7c7dfedc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4b1e0a269d2f47c88f44d4453df891f6",
            "value": "‚Äá33/33‚Äá[01:37&lt;00:00,‚Äá40.19s/it]"
          }
        },
        "03e2f614e062458882fd456db8fcd6d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bcb915ef4247e3b85bc95344566b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9d543fd9d4408bb5a77978792c5c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f26afb849168458fa3c4c3c36d0cd276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ae689d8479f4215b2b5e41e6562732d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76cdc9b7f67a40f19f853c0a7c7dfedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1e0a269d2f47c88f44d4453df891f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77306862fa714e6b9a6f684420b7a95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f7ab6ae32c5458ab05e295e705b5a81",
              "IPY_MODEL_3070715948db43e2b86ce566cd0dca21",
              "IPY_MODEL_89e43cdbceb94fa9a1a6963ec2fa65e4"
            ],
            "layout": "IPY_MODEL_43fb6f9890bd417f9a08b8f96def16ac"
          }
        },
        "8f7ab6ae32c5458ab05e295e705b5a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0aba55c6b1742409d1e99f7e5d0206d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_58ce78a5b2004b68b1e6ec0a35bd0766",
            "value": "Loading‚Äádataset‚Äáshards:‚Äá100%"
          }
        },
        "3070715948db43e2b86ce566cd0dca21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f22610e5a455464dbadede7e1700be37",
            "max": 34,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5f8712e5bf24ad0a7e640c0067e5915",
            "value": 34
          }
        },
        "89e43cdbceb94fa9a1a6963ec2fa65e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cee4177264d4e04a57cceb8ada5336d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24e2231ebc5e4e08a1dc0699b3009ddd",
            "value": "‚Äá34/34‚Äá[01:24&lt;00:00,‚Äá‚Äá1.19it/s]"
          }
        },
        "43fb6f9890bd417f9a08b8f96def16ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0aba55c6b1742409d1e99f7e5d0206d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58ce78a5b2004b68b1e6ec0a35bd0766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f22610e5a455464dbadede7e1700be37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f8712e5bf24ad0a7e640c0067e5915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cee4177264d4e04a57cceb8ada5336d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e2231ebc5e4e08a1dc0699b3009ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "721204a99f6f44aba56b04c7ac959603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5233537a87e741e7a460bbd00db7c16a",
              "IPY_MODEL_1004e155fe82412fbe8c35c8dfd049e3",
              "IPY_MODEL_59fb6b25bbab434788acce6e345059c4"
            ],
            "layout": "IPY_MODEL_732a2bd117f3428ead9bf56549c3905c"
          }
        },
        "5233537a87e741e7a460bbd00db7c16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b67aaef411d84023a40cdfb22fea8b57",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_276acad4b2b949e4bb398df701df2b43",
            "value": "Loading‚Äádataset‚Äáshards:‚Äá100%"
          }
        },
        "1004e155fe82412fbe8c35c8dfd049e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d3b0e16615d4ab98ceb366ead1dc337",
            "max": 45,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73c8f308ba9043878e85aac596c76ae4",
            "value": 45
          }
        },
        "59fb6b25bbab434788acce6e345059c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7832c2c1bbf641f998137b2cf71b1f1a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_10fd07f800884fbf86fd22b649ddfcbb",
            "value": "‚Äá45/45‚Äá[02:21&lt;00:00,‚Äá32.24s/it]"
          }
        },
        "732a2bd117f3428ead9bf56549c3905c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b67aaef411d84023a40cdfb22fea8b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276acad4b2b949e4bb398df701df2b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d3b0e16615d4ab98ceb366ead1dc337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c8f308ba9043878e85aac596c76ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7832c2c1bbf641f998137b2cf71b1f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fd07f800884fbf86fd22b649ddfcbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ae6a9b92366469a8e7fd4e0e23057c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d343780b136f498ab5a954fa1ab48dfa",
              "IPY_MODEL_b69a3e6f0285421cbeb5f22a1d52f47d",
              "IPY_MODEL_9e1cfa3f4f644640a1f48798d5dd5cb5"
            ],
            "layout": "IPY_MODEL_e83699e8ad754197b3f99ee0570ec813"
          }
        },
        "d343780b136f498ab5a954fa1ab48dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b947f6671c3462989ce6746e52e63df",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4cd6c16a164b48998fac0080dc5764b4",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "b69a3e6f0285421cbeb5f22a1d52f47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d56c77df79041a1bc500592226bb45b",
            "max": 30000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbc4a1cf9142491d8ddafa7d6b1e2ef9",
            "value": 30000
          }
        },
        "9e1cfa3f4f644640a1f48798d5dd5cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c696f92e594707920cd05e472143e5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bfaad65b023541da8437d163ce48f957",
            "value": "‚Äá30000/30000‚Äá[00:23&lt;00:00,‚Äá1321.45‚Äáexamples/s]"
          }
        },
        "e83699e8ad754197b3f99ee0570ec813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b947f6671c3462989ce6746e52e63df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd6c16a164b48998fac0080dc5764b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d56c77df79041a1bc500592226bb45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc4a1cf9142491d8ddafa7d6b1e2ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51c696f92e594707920cd05e472143e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfaad65b023541da8437d163ce48f957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44fb3c00cb6a4787a40b62f51648cc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c24cbe8339d4c649c23eac57dba3c92",
              "IPY_MODEL_8b0ad926c989427ca7212152c1754456",
              "IPY_MODEL_3580fb5a95da4677939a24be92017e4e"
            ],
            "layout": "IPY_MODEL_37c1743891054c5a866486fd072e9112"
          }
        },
        "2c24cbe8339d4c649c23eac57dba3c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d346855711d46809403c4004fef88b5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e5e70c906da44b93906e4ac21b028834",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "8b0ad926c989427ca7212152c1754456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df0467f05cf43b08ca8ee995e11b7f2",
            "max": 1650,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38730570595140bd8aa7217ed97b48d8",
            "value": 1650
          }
        },
        "3580fb5a95da4677939a24be92017e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83521a34b8d4461a89925aca3702f17e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_84ba61889cc6487cbcdb9fe1014ae5f8",
            "value": "‚Äá1650/1650‚Äá[00:01&lt;00:00,‚Äá936.52‚Äáexamples/s]"
          }
        },
        "37c1743891054c5a866486fd072e9112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d346855711d46809403c4004fef88b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e70c906da44b93906e4ac21b028834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8df0467f05cf43b08ca8ee995e11b7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38730570595140bd8aa7217ed97b48d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83521a34b8d4461a89925aca3702f17e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ba61889cc6487cbcdb9fe1014ae5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}